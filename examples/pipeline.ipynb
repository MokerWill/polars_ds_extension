{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from polars_ds.pipeline import Pipeline, Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builtin Pipeline Functions\n",
    "\n",
    "To run this demo: use the latest version of polars_ds. Or the latest commit if the latest version doesn't work.\n",
    "\n",
    "Need v0.5.2 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select\n",
    "*\n",
    ", 'TEST' as test_col\n",
    "from df\n",
    "where loan_amount is not null\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blueprint first. \n",
    "# A blueprint is a plan for a pipeline. No hard work will be done until the blueprint is materialized, which\n",
    "# is when the tranforms are fitted (e.g. scale learns the mean and std from base data)\n",
    "# If target is specified for the blueprint, target will be excluded from all transformations that require a fit,\n",
    "# and target will be auto-filled if the transformation requires a target field and when no target field is explicitly given.\n",
    "\n",
    "bp = (\n",
    "    Blueprint(df, name = \"example\", target = \"approved\") # You can optionally put target of the ML model here\n",
    "    # Select only the columns we need\n",
    "    .lowercase() # lowercase all columns\n",
    "    .sql_transform(sql) # Run a SQL transform on the df\n",
    "    # Say you want to remove a population for your data pipeline.\n",
    "    .filter( \n",
    "        \"city_category is not null\" # or equivalently, you can do: pl.col(\"city_category\").is_not_null()\n",
    "    )\n",
    "    .select(cs.numeric() | cs.by_name([\"gender\", \"employer_category1\", \"city_category\", \"test_col\"]))\n",
    "    # explicitly put target, since this is not the target for prediction. \n",
    "    # Use a linear regression with x1 = var1, x2=existing_emi to predict missing values in loan_period\n",
    "    .linear_impute(features = [\"var1\", \"existing_emi\"], target = \"loan_period\") \n",
    "    .impute([\"existing_emi\"], method = \"median\")\n",
    "    .append_expr( # generate some features\n",
    "        pl.col(\"existing_emi\").log1p().alias(\"existing_emi_log1p\"),\n",
    "        pl.col(\"loan_amount\").log1p().alias(\"loan_amount_log1p\"),\n",
    "        pl.col(\"loan_amount\").clip(lower_bound = 0, upper_bound = 1000).alias(\"loan_amount_log1p_clipped\"),\n",
    "        pl.col(\"loan_amount\").sqrt().alias(\"loan_amount_sqrt\"),\n",
    "        pl.col(\"loan_amount\").shift(-1).alias(\"loan_amount_lag_1\") # any kind of lag transform\n",
    "    )\n",
    "    .scale( # target is numerical, but will be excluded automatically because bp is initialzied with a target\n",
    "        cs.numeric().exclude([\"var1\", \"existing_emi_log1p\"]), method = \"standard\"\n",
    "    ) # Scale the columns up to this point. The columns below won't be scaled\n",
    "    .append_expr(\n",
    "        # Add missing flags\n",
    "        pl.col(\"employer_category1\").is_null().cast(pl.UInt8).alias(\"employer_category1_is_missing\")\n",
    "    )\n",
    "    .one_hot_encode(\"gender\", drop_first=True)\n",
    "    .woe_encode(\"city_category\") # No need to specify target because we initialized bp with a target\n",
    "    .target_encode(\"employer_category1\", min_samples_leaf = 20, smoothing = 10.0) # same as above\n",
    "    .shrink_dtype(force_f32 = True) # shrink dtype to smallest possible. Force floats to be f32\n",
    ")\n",
    "\n",
    "print(bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the blueprint\n",
    "pipe:Pipeline = bp.materialize()\n",
    "# Text representation of the pipeline\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want separation between features (X) and target (y)\n",
    "# you can run the following:\n",
    "# df_x, df_y = pipe.transform(df, separate=True)\n",
    "\n",
    "df_transformed = pipe.transform(df)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty. Because we filtered this to not null.\n",
    "df_transformed.filter(\n",
    "    pl.col(\"city_category\").is_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty. Because we filtered this to not null in the SQL\n",
    "df_transformed.filter(\n",
    "    pl.col(\"loan_amount\").is_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization Methods\n",
    "\n",
    "Pickle + JSON support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# The pipe object can be pickled\n",
    "with open(\"pipe.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pipe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pipe.pickle\", \"rb\") as f:\n",
    "    pipe2 = pickle.load(f)\n",
    "\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed_2 = pipe2.transform(df)\n",
    "df_transformed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars.testing import assert_frame_equal\n",
    "# True\n",
    "assert_frame_equal(df_transformed, df_transformed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the pipeline as JSON\n",
    "\n",
    "pipe.to_json(\"test.json\")\n",
    "pipe3 = Pipeline.from_json(\"test.json\")\n",
    "# True\n",
    "assert_frame_equal(df_transformed, pipe3.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tranformations in Pipeline\n",
    "\n",
    "Need version >= v0.4.6 (Not released yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"../examples/dependency.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "# Any custom function must satistfy the following function signature:\n",
    "# func(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], ...) -> List[pl.Expr]\n",
    "# where ... means kwargs\n",
    "# Here is a custom imputer\n",
    "\n",
    "def smallest_abs_impute(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], epsilon:float = 0.01) -> List[pl.Expr]:\n",
    "    \"\"\"\n",
    "    Imputes columns by the min of the absolute values for c in columns, plus epsilon.\n",
    "    \"\"\"\n",
    "    temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "    return [pl.col(c).fill_null(m) for c, m in zip(cols, temp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = (\n",
    "    Blueprint(df, name = \"example\", target = \"approved\")\n",
    "    .lowercase() # lowercase all columns\n",
    "    .append_fit_func(smallest_abs_impute, [\"var1\", \"existing_emi\", \"loan_amount\"], epsilon = 0.5)\n",
    "    # Use append_fit_func for custom transforms\n",
    ")\n",
    "# Notice that the value to impute is correct, it is 0.5, because the min abs of the columns are 0.\n",
    "pipe:Pipeline = bp.materialize()\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.transform(df).null_count().select([\"var1\", \"existing_emi\", \"loan_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scriptable Steps in Pipeline\n",
    "\n",
    "What is a scriptable step? It means we can encode steps in a json or yaml file easily. As long as we can turn the text into \n",
    "a valid Python dictionary, the step (the transformation) can be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    pl.col(\"Existing_EMI\").null_count() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = Blueprint(df, name = \"example\", target = \"approved\")\n",
    "\n",
    "# Takes in a dict with 3 fields: `name`, `args`, and `kwargs`. \n",
    "# Args and kwargs are optional depending on whether the method call needs certain arguments.\n",
    "step_dict_1 = {\n",
    "    \"name\": \"impute\",\n",
    "    \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"method\": \"median\"} \n",
    "}\n",
    "\n",
    "step_dict_2 = {\n",
    "    \"name\": \"does_not_exist\",\n",
    "    \"kwargs\": {\"test\": 1} \n",
    "}\n",
    "\n",
    "# filter_step = {\n",
    "#     \"name\": \"filter\",\n",
    "#     \"args\": [\"Employer_Category1 is not null\"]\n",
    "# }\n",
    "\n",
    "bp.append_step_from_dict(\n",
    "    step_dict_1\n",
    ")\n",
    "\n",
    "# .append_step_from_dict(\n",
    "#     filter_step\n",
    "# )\n",
    "\n",
    "# bp.append_step_from_dict(step_dict_2) # Will error\n",
    "pipe = bp.materialize()\n",
    "# \n",
    "df_transformed = pipe.transform(df)\n",
    "df_transformed.select(\n",
    "    pl.col(\"Existing_EMI\").null_count() # Imputed. So 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transforms as a Scriptable Step in Pipeline\n",
    "\n",
    "You need to inherit the blueprint class. Once the blueprint is materialized (learned). You do not need this class any more, because the \"learned\" info should all be encoded as Polars expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polars_ds.pipeline import Blueprint, FitStep\n",
    "from typing import Self, Union, List\n",
    "from functools import partial\n",
    "\n",
    "def smallest_abs_impute(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str], epsilon:float = 0.01) -> List[pl.Expr]:\n",
    "    \"\"\"\n",
    "    Imputes columns by the min of the absolute values for c in columns, plus epsilon.\n",
    "    \"\"\"\n",
    "    temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "    return [pl.col(c).fill_null(m).name.suffix(\"_imputed\") for c, m in zip(cols, temp)]\n",
    "\n",
    "class ExtendedBlueprint(Blueprint):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def smallest_abs_impute(self, cols: List[str], epsilon:float = 0.01) -> Self:\n",
    "        # bind all arguments, except df and cols.\n",
    "        # If you don't want to use partial from functool, you can define an inner function\n",
    "        partial_func = partial(smallest_abs_impute, epsilon=epsilon)\n",
    "        self._steps.append(\n",
    "            FitStep(partial_func, cols, self.exclude)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def smallest_abs_impute2(self, cols: List[str], epsilon:float = 0.01) -> Self:\n",
    "        # bind all arguments, except df and cols.\n",
    "        # Example of using an inner function\n",
    "        def inner_func(df:Union[pl.DataFrame, pl.LazyFrame], cols: List[str]) -> List[pl.Expr]:\n",
    "            temp = df.lazy().select(pl.col(cols).abs().min() + epsilon).collect().row(0)\n",
    "            return [pl.col(c).fill_null(m).name.suffix(\"_imputed2\") for c, m in zip(cols, temp)]\n",
    "\n",
    "        self._steps.append(\n",
    "            FitStep(inner_func, cols, self.exclude)\n",
    "        )\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = ExtendedBlueprint(df, name = \"example\", target = \"approved\")\n",
    "\n",
    "# Takes in a dict with 3 fields: `name`, `args`, and `kwargs`. \n",
    "# Args and kwargs are optional depending on whether the method call needs certain arguments.\n",
    "step_dict_1 = {\n",
    "    \"name\": \"smallest_abs_impute\",\n",
    "    \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"epsilon\": 0.01} \n",
    "}\n",
    "\n",
    "step_dict_2 = {\n",
    "    \"name\": \"smallest_abs_impute2\",\n",
    "    \"kwargs\": {\"cols\": [\"Existing_EMI\"], \"epsilon\": 0.01} \n",
    "}\n",
    "\n",
    "bp.append_step_from_dict(\n",
    "    step_dict_1\n",
    ").append_step_from_dict(\n",
    "    step_dict_2\n",
    ")\n",
    "\n",
    "pipe = bp.materialize()\n",
    "df_transformed = pipe.transform(df)\n",
    "\n",
    "df_transformed.with_columns(\n",
    "    impute_value = pl.col(\"Existing_EMI\").abs().min() + 0.01\n",
    ").filter(\n",
    "    pl.col(\"Existing_EMI\").is_null()\n",
    ").select(\n",
    "    pl.col(\"Existing_EMI\"),\n",
    "    pl.col(\"Existing_EMI_imputed\"),\n",
    "    pl.col(\"Existing_EMI_imputed2\"),\n",
    "    pl.col(\"impute_value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
